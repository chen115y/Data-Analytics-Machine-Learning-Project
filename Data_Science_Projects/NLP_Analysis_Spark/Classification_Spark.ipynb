{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction:-Comparison-of-NLP-Learning-for-Job-Descriptions\" data-toc-modified-id=\"Introduction:-Comparison-of-NLP-Learning-for-Job-Descriptions-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction: Comparison of NLP Learning for Job Descriptions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dataset\" data-toc-modified-id=\"Dataset-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Dataset</a></span></li><li><span><a href=\"#Python-Library\" data-toc-modified-id=\"Python-Library-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Python Library</a></span></li></ul></li><li><span><a href=\"#Load-Data-Set\" data-toc-modified-id=\"Load-Data-Set-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Load Data Set</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Load\" data-toc-modified-id=\"Data-Load-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Data Load</a></span></li><li><span><a href=\"#Data-Clean-up\" data-toc-modified-id=\"Data-Clean-up-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Data Clean-up</a></span></li></ul></li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Feature Engineering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tokenization\" data-toc-modified-id=\"Tokenization-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Tokenization</a></span></li><li><span><a href=\"#Stopword-Removal\" data-toc-modified-id=\"Stopword-Removal-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Stopword Removal</a></span></li><li><span><a href=\"#Lemmatization\" data-toc-modified-id=\"Lemmatization-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Lemmatization</a></span></li><li><span><a href=\"#Hashing-TF-(Count)-Vectors\" data-toc-modified-id=\"Hashing-TF-(Count)-Vectors-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Hashing TF (Count) Vectors</a></span></li><li><span><a href=\"#TFIDF-Vectors\" data-toc-modified-id=\"TFIDF-Vectors-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>TFIDF Vectors</a></span></li><li><span><a href=\"#Word-Embedding-Vectors\" data-toc-modified-id=\"Word-Embedding-Vectors-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Word Embedding Vectors</a></span></li></ul></li><li><span><a href=\"#Training-and-Testing-Data-Sets-Preparation\" data-toc-modified-id=\"Training-and-Testing-Data-Sets-Preparation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Training and Testing Data Sets Preparation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Word2Vec\" data-toc-modified-id=\"Word2Vec-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Word2Vec</a></span></li><li><span><a href=\"#TFIDF\" data-toc-modified-id=\"TFIDF-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>TFIDF</a></span></li><li><span><a href=\"#Hashing-TF\" data-toc-modified-id=\"Hashing-TF-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Hashing TF</a></span></li><li><span><a href=\"#Setup-Encoding-and-Feature-Assembler\" data-toc-modified-id=\"Setup-Encoding-and-Feature-Assembler-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Setup Encoding and Feature Assembler</a></span></li><li><span><a href=\"#Setup-Model-Evaluator\" data-toc-modified-id=\"Setup-Model-Evaluator-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Setup Model Evaluator</a></span></li></ul></li><li><span><a href=\"#Classifications\" data-toc-modified-id=\"Classifications-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Classifications</a></span><ul class=\"toc-item\"><li><span><a href=\"#Classification-with-Naive-Bayes\" data-toc-modified-id=\"Classification-with-Naive-Bayes-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Classification with Naive Bayes</a></span><ul class=\"toc-item\"><li><span><a href=\"#TFIDF\" data-toc-modified-id=\"TFIDF-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>TFIDF</a></span></li><li><span><a href=\"#Hashing-TF\" data-toc-modified-id=\"Hashing-TF-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Hashing TF</a></span></li><li><span><a href=\"#Word2Vec---Failed-due-to-negative-feature-values\" data-toc-modified-id=\"Word2Vec---Failed-due-to-negative-feature-values-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;</span>Word2Vec - Failed due to negative feature values</a></span></li></ul></li><li><span><a href=\"#Classification-with-Random-Forest\" data-toc-modified-id=\"Classification-with-Random-Forest-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Classification with Random Forest</a></span><ul class=\"toc-item\"><li><span><a href=\"#Word2Vec\" data-toc-modified-id=\"Word2Vec-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Word2Vec</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cross-Validation\" data-toc-modified-id=\"Cross-Validation-5.2.1.1\"><span class=\"toc-item-num\">5.2.1.1&nbsp;&nbsp;</span>Cross Validation</a></span></li></ul></li><li><span><a href=\"#TFIDF\" data-toc-modified-id=\"TFIDF-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;</span>TFIDF</a></span></li><li><span><a href=\"#Hashing-TF\" data-toc-modified-id=\"Hashing-TF-5.2.3\"><span class=\"toc-item-num\">5.2.3&nbsp;&nbsp;</span>Hashing TF</a></span></li></ul></li><li><span><a href=\"#Classification-with-Dense-Neural-Network\" data-toc-modified-id=\"Classification-with-Dense-Neural-Network-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Classification with Dense Neural Network</a></span><ul class=\"toc-item\"><li><span><a href=\"#Word2Vec\" data-toc-modified-id=\"Word2Vec-5.3.1\"><span class=\"toc-item-num\">5.3.1&nbsp;&nbsp;</span>Word2Vec</a></span></li></ul></li></ul></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Summary</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Comparison of NLP Learning for Job Descriptions\n",
    "\n",
    "In this notebook, I will demonstrate how to process unstructure text feature data by using natural language processing (NLP) technologies and then train a classification model with PySpark library.  The major steps include feature engineering and  multi-class classification model training. During feature engineering, I am going to compare several NLP vetorization methods:\n",
    "\n",
    "1. __Hashing TF__\n",
    "2. __TFIDF__\n",
    "3. __Word Embedding with Word2Vec__\n",
    "\n",
    "During the classification model training, I will compare several classification algorithms:\n",
    "1. __Naive Bayes__\n",
    "2. __Random Forest__\n",
    "3. __Neural Network__\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset is a historical data of job descriptions stored as \"job_descriptions_Labeled.csv\" file.\n",
    "\n",
    "## Python Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T18:57:26.928539Z",
     "start_time": "2019-10-17T18:57:21.554835Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/local/spark/spark-2.4.3-bin-hadoop2.7')\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "# Set up a spark session with leveraging all available CPUs\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master('local[*]')\\\n",
    "        .appName(\"NLP\") \\\n",
    "        .config(\"spark.some.config.option\", \"setting\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T18:57:29.199277Z",
     "start_time": "2019-10-17T18:57:29.195729Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load pyspark SQL library\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType, IntegerType, ArrayType, FloatType, DoubleType\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "# Load pyspark ML Feature processing library\n",
    "from pyspark.ml.feature import StopWordsRemover \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import Word2VecModel\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "# Load pyspark ML classification library\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T18:57:32.087159Z",
     "start_time": "2019-10-17T18:57:31.252084Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pandas and numpy for converting from Spark dataframe into Pandas dataframe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Make the random numbers predictable\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T18:57:34.681966Z",
     "start_time": "2019-10-17T18:57:33.636420Z"
    }
   },
   "outputs": [],
   "source": [
    "# for print highlighted text\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    \n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T18:57:37.358005Z",
     "start_time": "2019-10-17T18:57:36.460428Z"
    }
   },
   "outputs": [],
   "source": [
    "# I am going to use NLTK for stopwords removal and lemmatization\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T18:54:48.046807Z",
     "start_time": "2019-10-10T18:54:47.753703Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ivan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ivan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw to /home/ivan/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T18:57:48.162522Z",
     "start_time": "2019-10-17T18:57:40.961934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- req_guid: string (nullable = true)\n",
      " |-- client_job_title: string (nullable = true)\n",
      " |-- labeled: string (nullable = true)\n",
      " |-- req_qualified_date: string (nullable = true)\n",
      " |-- codes: string (nullable = true)\n",
      " |-- codes_description: string (nullable = true)\n",
      " |-- codes_level: string (nullable = true)\n",
      " |-- job_description: string (nullable = true)\n",
      "\n",
      "The total of job descriptions is:  10543\n"
     ]
    }
   ],
   "source": [
    "# Read all data\n",
    "df = spark.read.csv('./job_descriptions_Labeled.csv', header='true', inferSchema='true')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T18:57:56.171096Z",
     "start_time": "2019-10-17T18:57:55.937443Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove any row without job description\n",
    "df = df.where(col('job_description').isNotNull())\n",
    "print(\"The total of job descriptions is: \", df.count())\n",
    "# Check some samples\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "In this step, raw text data will be transformed into feature vectors. We will implement the following steps in order to obtain relevant features from our dataset.\n",
    "\n",
    "* Tokenizing\n",
    "* Remove stop words\n",
    "* Lemmatization (not stem since stemming can reduce the interpretability) \n",
    "* Hashing TF\n",
    "* TFIDF\n",
    "* Word Embeddings as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization is the process by dividing the quantity of text into smaller parts called tokens so that each token can be further treated for machine learning purposes. A token can be a character, a word, a sentence or a paragraph. In this notebook, I only consider words as tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T18:57:59.648588Z",
     "start_time": "2019-10-17T18:57:59.567980Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize the job description\n",
    "df = df.withColumn('job_description',split(col('job_description'),' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T18:58:01.766735Z",
     "start_time": "2019-10-17T18:58:01.592306Z"
    }
   },
   "outputs": [],
   "source": [
    "df.select('job_description','client_job_title', 'labeled').head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword Removal\n",
    "\n",
    "A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a NLP program has been programmed to ignore. In this notebook, I will use NLTK stop words dataset to remove any stop words in job description field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T18:58:04.451244Z",
     "start_time": "2019-10-17T18:58:04.384603Z"
    }
   },
   "outputs": [],
   "source": [
    "# Stop Word list from NLTK library\n",
    "stop_words = stopwords.words('english')\n",
    "# Define stop word removal function\n",
    "def removeStopWords(x):\n",
    "        return [w.lower() for w in x if (w not in stop_words) and (w != '') and (w is not None)]\n",
    "removeStopWords_udf = udf(lambda x: removeStopWords(x) if x is not None else None, ArrayType(StringType()))\n",
    "\n",
    "# Apply function to each job description\n",
    "df = df.withColumn(\"job_description\", removeStopWords_udf(col('job_description')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T18:58:08.900238Z",
     "start_time": "2019-10-17T18:58:07.664220Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.select('job_description', 'labeled').head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatisation (or lemmatization) in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form. For example, in English, the verb 'to walk' may appear as 'walk', 'walked', 'walks', 'walking'. The base form, 'walk', that one might look up in a dictionary, is called the lemma for the word. I will use NLTK lemmatization function to convert words into their lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T18:58:11.644229Z",
     "start_time": "2019-10-17T18:58:11.604876Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a lemmatization function\n",
    "def lemma(x):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(w.lower(), pos='v') for w in x if (w != '') and (w is not None)]\n",
    "\n",
    "get_lemma_udf = udf(lemma, ArrayType(StringType()))\n",
    "\n",
    "df = df.withColumn(\"job_description\", get_lemma_udf(col('job_description')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T18:58:17.995602Z",
     "start_time": "2019-10-17T18:58:14.423971Z"
    }
   },
   "outputs": [],
   "source": [
    "documentDF = df.select('req_guid', 'job_description', 'labeled')\n",
    "documentDF.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashing TF (Count) Vectors\n",
    "\n",
    "HashingTF is a Transformer which takes sets of terms and converts those sets into fixed-length feature vectors. In text processing, a “set of terms” might be a bag of words. HashingTF utilizes the hashing trick. A raw feature is mapped into an index (term) by applying a hash function. Then term frequencies are calculated based on the mapped indices. This approach avoids the need to compute a global term-to-index map, which can be expensive for a large corpus, but it suffers from potential hash collisions, where different raw features may become the same term after hashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T18:58:24.578573Z",
     "start_time": "2019-10-17T18:58:20.559473Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate top 20 TF vectors\n",
    "hashingTF = HashingTF(inputCol=\"job_description\", outputCol=\"job_description_vectors\", numFeatures=20)\n",
    "result_tf = hashingTF.transform(documentDF)\n",
    "# Show some sample results\n",
    "result_tf.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF Vectors\n",
    "\n",
    "TF-IDF is an abbreviation for Term Frequency-Inverse Document Frequency and is a very common algorithm to transform text into a meaningful representation of numbers. The technique is widely used to extract features across various NLP applications. Spark.mllib's IDF implementation provides an option for ignoring terms which occur in less than a minimum number of documents.\n",
    " In such cases, the IDF for these terms is set to 0. This feature can be used by passing the minDocFreq value to the IDF constructor. \n",
    " ```\n",
    " idfIgnore = IDF(inputCol=\"TFs\", outputCol=\"TFIDFs\", minDocFreq=2).fit(TFData)  \n",
    " tfidfIgnore = idfIgnore.transform(TFData)\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T18:58:36.190814Z",
     "start_time": "2019-10-17T18:58:27.926714Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate top 20 TF vectors\n",
    "hashingTF = HashingTF(inputCol=\"job_description\", outputCol=\"TFs\", numFeatures=20)\n",
    "TFData = hashingTF.transform(documentDF)\n",
    "\n",
    "idf = IDF(inputCol=\"TFs\", outputCol=\"job_description_vectors\")\n",
    "idfModel = idf.fit(TFData)\n",
    "result_tfidf = idfModel.transform(TFData)\n",
    "\n",
    "# Show some sample results\n",
    "result_tfidf.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding Vectors\n",
    "\n",
    "A word embedding is a form of representing words and documents using a dense vector representation. The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used. Word embeddings can be trained using the input texts. One can read more about word embeddings [here](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/) and [here](https://jalammar.github.io/illustrated-word2vec/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T18:58:40.983043Z",
     "start_time": "2019-10-17T18:58:40.963551Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec_jd = Word2Vec(vectorSize=100, minCount=1, inputCol=\"job_description\", outputCol=\"job_description_vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T18:59:17.581621Z",
     "start_time": "2019-10-17T18:58:43.644154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- req_guid: string (nullable = true)\n",
      " |-- job_description: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- labeled: string (nullable = true)\n",
      " |-- job_description_vectors: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train word2vec model\n",
    "model_jd = word2Vec_jd.fit(documentDF)\n",
    "# Transform job descriptions into vectors based on trained word2vec model. \n",
    "# In Spark, average of vectors is used for transformation.\n",
    "result_jd = model_jd.transform(documentDF)\n",
    "result_jd.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T18:59:23.956888Z",
     "start_time": "2019-10-17T18:59:22.955062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|       word|              vector|\n",
      "+-----------+--------------------+\n",
      "|   incident|[-0.0152254654094...|\n",
      "|    serious|[0.04986744374036...|\n",
      "|   youthful|[0.02353858202695...|\n",
      "|     comply|[0.50005894899368...|\n",
      "|        ebb|[4.60543902590870...|\n",
      "|        dns|[0.00637202151119...|\n",
      "|    liquent|[0.00463599432259...|\n",
      "| compliment|[0.00900448951870...|\n",
      "|    sectors|[-0.0562520958483...|\n",
      "|  ascension|[0.07105297595262...|\n",
      "|     hourly|[0.21412841975688...|\n",
      "|salesperson|[-0.0110264718532...|\n",
      "|    speaker|[-0.0055515342392...|\n",
      "|  cabinetry|[0.01142293773591...|\n",
      "|malfunction|[-0.0290974136441...|\n",
      "|   terrible|[-0.0088737308979...|\n",
      "|       lion|[0.00667591439560...|\n",
      "|       rate|[0.36700674891471...|\n",
      "|        205|[0.08493795990943...|\n",
      "|       2014|[0.02736475504934...|\n",
      "+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show some vector samples\n",
    "model_jd.getVectors().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T00:59:38.222967Z",
     "start_time": "2019-10-15T00:59:34.340212Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# The model can be saved in a binary file\n",
    "model_jd.save(\"word2vec_model.bin\")\n",
    "# And it can be loaded later on\n",
    "model_jd = Word2VecModel.load(\"develper_word2vec_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T18:59:29.173724Z",
     "start_time": "2019-10-17T18:59:29.155892Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get word2vec data ste\n",
    "result_w2v = result_jd.select('req_guid','job_description_vectors', 'labeled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T18:59:31.936015Z",
     "start_time": "2019-10-17T18:59:31.748016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10543\n"
     ]
    }
   ],
   "source": [
    "print(result_w2v.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T18:59:34.216148Z",
     "start_time": "2019-10-17T18:59:34.095642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+\n",
      "|         word|         similarity|\n",
      "+-------------+-------------------+\n",
      "|         reps|0.46157094836235046|\n",
      "|          rep|0.44627615809440613|\n",
      "|cooperatively|0.42983150482177734|\n",
      "|    overnight| 0.4041067957878113|\n",
      "|        mitel| 0.4008449912071228|\n",
      "+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find some similar words\n",
    "model_jd.findSynonyms('representatives',5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing Data Sets Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T19:01:44.827214Z",
     "start_time": "2019-10-17T19:01:33.507922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7448\n",
      "3095\n"
     ]
    }
   ],
   "source": [
    "# Split dataset randomly into Training and Test sets. Set seed for reproducibility\n",
    "(trainingData_w2v, testData_w2v) = result_w2v.randomSplit([0.7, 0.3], seed = 100)\n",
    "\n",
    "trainingData_w2v.cache()\n",
    "testData_w2v.cache()\n",
    "\n",
    "print(trainingData_w2v.count())\n",
    "print(testData_w2v.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T19:02:05.855462Z",
     "start_time": "2019-10-17T19:01:46.422123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7448\n",
      "3095\n"
     ]
    }
   ],
   "source": [
    "# Split dataset randomly into Training and Test sets. Set seed for reproducibility\n",
    "(trainingData_tfidf, testData_tfidf) = result_tfidf.randomSplit([0.7, 0.3], seed = 100)\n",
    "\n",
    "trainingData_tfidf.cache()\n",
    "testData_tfidf.cache()\n",
    "\n",
    "print(trainingData_tfidf.count())\n",
    "print(testData_tfidf.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashing TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T19:02:21.610266Z",
     "start_time": "2019-10-17T19:02:07.607542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7448\n",
      "3095\n"
     ]
    }
   ],
   "source": [
    "# Split dataset randomly into Training and Test sets. Set seed for reproducibility\n",
    "(trainingData_tf, testData_tf) = result_tf.randomSplit([0.7, 0.3], seed = 100)\n",
    "\n",
    "trainingData_tf.cache()\n",
    "testData_tf.cache()\n",
    "\n",
    "print(trainingData_tf.count())\n",
    "print(testData_tf.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Encoding and Feature Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T19:02:28.039398Z",
     "start_time": "2019-10-17T19:02:28.018650Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert target into numerical categories\n",
    "labelIndexer = StringIndexer(inputCol=\"labeled\", outputCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T19:02:30.185703Z",
     "start_time": "2019-10-17T19:02:30.168749Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define an assembler\n",
    "vecAssembler = VectorAssembler(inputCols=[\"job_description_vectors\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Model Evaluator\n",
    "\n",
    "To evaluate the classification model, I will be making use of the Evaluator in MulticlassClassification. \n",
    "Note that f1-score is the default metric for the MulticlassClassificationEvaluator. \n",
    "There are other choices for evaluations metrics that can be found in the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T19:14:24.889544Z",
     "start_time": "2019-10-17T19:14:24.871796Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T03:55:44.839737Z",
     "start_time": "2019-10-17T03:55:44.829162Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a NaiveBayes model\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T03:55:48.510856Z",
     "start_time": "2019-10-17T03:55:47.561792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy:  0.9163166397415186\n"
     ]
    }
   ],
   "source": [
    "# Chain labelIndexer, vecAssembler and NBmodel in a \n",
    "pipeline_tfidf = Pipeline(stages=[labelIndexer, vecAssembler, nb])\n",
    "\n",
    "# Run stages in pipeline and train model\n",
    "model_tfidf = pipeline_tfidf.fit(trainingData_tfidf)\n",
    "\n",
    "# Make predictions on testData so we can measure the accuracy of our model on new data\n",
    "predictions_tfidf = model_tfidf.transform(testData_tfidf)\n",
    "\n",
    "# Evaluation of model\n",
    "accuracy = evaluator.evaluate(predictions_tfidf)\n",
    "print(\"Model Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T03:57:02.556037Z",
     "start_time": "2019-10-17T03:57:01.893817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy:  0.8781906300484653\n"
     ]
    }
   ],
   "source": [
    "# Chain labelIndexer, vecAssembler and NBmodel in a \n",
    "pipeline_tf = Pipeline(stages=[labelIndexer, vecAssembler, nb])\n",
    "\n",
    "# Run stages in pipeline and train model\n",
    "model_tf = pipeline_tf.fit(trainingData_tf)\n",
    "\n",
    "# Make predictions on testData so we can measure the accuracy of our model on new data\n",
    "predictions_tf = model_tf.transform(testData_tf)\n",
    "\n",
    "# Evaluation of model\n",
    "accuracy = evaluator.evaluate(predictions_tf)\n",
    "print(\"Model Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec - Failed due to negative feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T04:00:05.735948Z",
     "start_time": "2019-10-17T04:00:05.209113Z"
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o21135.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 1340.0 failed 1 times, most recent failure: Lost task 2.0 in stage 1340.0 (TID 3358, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: Naive Bayes requires nonnegative feature values but found [-0.0813618281004723,0.032609838120934424,-0.03778935440522917,-0.10364296923826048,0.05866389085205683,0.043506006094651446,-0.020035373965456953,6.58658283290881E-4,-0.0027596470361797485,-0.036454196965530365,0.03091503046693275,-0.024912650818551745,0.05985457125237634,0.013168900686270166,0.07241504154842195,-0.06052869467266434,-0.012242375916177353,-0.06086302072968507,0.10797833527469139,0.05335005043584821,0.1044013091159286,0.10547053876575245,-0.05264355177031004,-0.029077896800773422,0.10194337404492787,-0.04681117238807167,-0.08145223454122844,-0.027785475441538268,-0.034363409402769406,0.014576943840650126,0.02864424111375715,0.023761392692862034,-0.028002761636703642,0.0017686769311948474,0.05585292022628574,-0.10172763996524736,0.02417167796087343,0.07091075816466866,-0.030473114661723484,0.05108258736165522,-0.0777270660795228,-0.025130549630523812,-0.029875145372961712,0.019255216717706494,0.013117203459757526,-0.07160050556033871,0.044524972286105136,-0.056780038227199923,-0.007080972116158972,-0.0016841881520650891,0.009087646986509478,0.026151567096497085,0.021114954326253082,-0.1269340146902196,-0.005676510123479137,0.05398663980778047,0.0026529734808041954,0.01208363535210252,0.0318425986756771,-4.406660469654991E-4,-0.0038469612954941113,-0.03372569480060795,-0.09066216464067788,0.007190218382632782,-0.03312360850446397,0.027211981990677845,0.02483781263215987,0.01630497156834598,-0.047580174914274236,0.01090316762362585,0.02743201835289446,0.0017425844433641624,-0.03580083113673365,0.0067454096597218665,-0.02806762962427456,0.021011611901908613,0.0977711274612121,0.05840684828953547,-0.027726174517653794,0.04413483504829983,-0.02351571827941455,-0.03264092489214733,-0.0543707497177651,0.025159678338664084,-9.86891551630918E-4,-0.029496228709873788,0.016395744569139382,0.13333857845316024,0.11650330971975183,0.030600413567722275,-0.032125101438459536,-0.013714076044527965,0.03599450626110368,-0.028204840349052424,0.04580524806061835,-0.039912409137096524,-0.11205664575763241,-0.05637828975869884,0.16036292395353463,-0.03346267257201848].\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.classification.NaiveBayes$.requireNonnegativeValues(NaiveBayes.scala:235)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$4.apply(NaiveBayes.scala:144)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$4.apply(NaiveBayes.scala:144)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$7.apply(NaiveBayes.scala:168)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$7.apply(NaiveBayes.scala:166)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$aggregateByKey$1$$anonfun$apply$6.apply(PairRDDFunctions.scala:172)\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:189)\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:188)\n\tat org.apache.spark.util.collection.AppendOnlyMap.changeValue(AppendOnlyMap.scala:144)\n\tat org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(SizeTrackingAppendOnlyMap.scala:32)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:194)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1.apply(NaiveBayes.scala:176)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1.apply(NaiveBayes.scala:129)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:183)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:183)\n\tat org.apache.spark.ml.classification.NaiveBayes.trainWithLabelCheck(NaiveBayes.scala:129)\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:118)\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:78)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Naive Bayes requires nonnegative feature values but found [-0.0813618281004723,0.032609838120934424,-0.03778935440522917,-0.10364296923826048,0.05866389085205683,0.043506006094651446,-0.020035373965456953,6.58658283290881E-4,-0.0027596470361797485,-0.036454196965530365,0.03091503046693275,-0.024912650818551745,0.05985457125237634,0.013168900686270166,0.07241504154842195,-0.06052869467266434,-0.012242375916177353,-0.06086302072968507,0.10797833527469139,0.05335005043584821,0.1044013091159286,0.10547053876575245,-0.05264355177031004,-0.029077896800773422,0.10194337404492787,-0.04681117238807167,-0.08145223454122844,-0.027785475441538268,-0.034363409402769406,0.014576943840650126,0.02864424111375715,0.023761392692862034,-0.028002761636703642,0.0017686769311948474,0.05585292022628574,-0.10172763996524736,0.02417167796087343,0.07091075816466866,-0.030473114661723484,0.05108258736165522,-0.0777270660795228,-0.025130549630523812,-0.029875145372961712,0.019255216717706494,0.013117203459757526,-0.07160050556033871,0.044524972286105136,-0.056780038227199923,-0.007080972116158972,-0.0016841881520650891,0.009087646986509478,0.026151567096497085,0.021114954326253082,-0.1269340146902196,-0.005676510123479137,0.05398663980778047,0.0026529734808041954,0.01208363535210252,0.0318425986756771,-4.406660469654991E-4,-0.0038469612954941113,-0.03372569480060795,-0.09066216464067788,0.007190218382632782,-0.03312360850446397,0.027211981990677845,0.02483781263215987,0.01630497156834598,-0.047580174914274236,0.01090316762362585,0.02743201835289446,0.0017425844433641624,-0.03580083113673365,0.0067454096597218665,-0.02806762962427456,0.021011611901908613,0.0977711274612121,0.05840684828953547,-0.027726174517653794,0.04413483504829983,-0.02351571827941455,-0.03264092489214733,-0.0543707497177651,0.025159678338664084,-9.86891551630918E-4,-0.029496228709873788,0.016395744569139382,0.13333857845316024,0.11650330971975183,0.030600413567722275,-0.032125101438459536,-0.013714076044527965,0.03599450626110368,-0.028204840349052424,0.04580524806061835,-0.039912409137096524,-0.11205664575763241,-0.05637828975869884,0.16036292395353463,-0.03346267257201848].\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.classification.NaiveBayes$.requireNonnegativeValues(NaiveBayes.scala:235)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$4.apply(NaiveBayes.scala:144)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$4.apply(NaiveBayes.scala:144)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$7.apply(NaiveBayes.scala:168)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$7.apply(NaiveBayes.scala:166)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$aggregateByKey$1$$anonfun$apply$6.apply(PairRDDFunctions.scala:172)\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:189)\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:188)\n\tat org.apache.spark.util.collection.AppendOnlyMap.changeValue(AppendOnlyMap.scala:144)\n\tat org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(SizeTrackingAppendOnlyMap.scala:32)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:194)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-5d77beb22b32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Run stages in pipeline and train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel_w2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline_w2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainingData_w2v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Make predictions on testData so we can measure the accuracy of our model on new data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o21135.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 1340.0 failed 1 times, most recent failure: Lost task 2.0 in stage 1340.0 (TID 3358, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: Naive Bayes requires nonnegative feature values but found [-0.0813618281004723,0.032609838120934424,-0.03778935440522917,-0.10364296923826048,0.05866389085205683,0.043506006094651446,-0.020035373965456953,6.58658283290881E-4,-0.0027596470361797485,-0.036454196965530365,0.03091503046693275,-0.024912650818551745,0.05985457125237634,0.013168900686270166,0.07241504154842195,-0.06052869467266434,-0.012242375916177353,-0.06086302072968507,0.10797833527469139,0.05335005043584821,0.1044013091159286,0.10547053876575245,-0.05264355177031004,-0.029077896800773422,0.10194337404492787,-0.04681117238807167,-0.08145223454122844,-0.027785475441538268,-0.034363409402769406,0.014576943840650126,0.02864424111375715,0.023761392692862034,-0.028002761636703642,0.0017686769311948474,0.05585292022628574,-0.10172763996524736,0.02417167796087343,0.07091075816466866,-0.030473114661723484,0.05108258736165522,-0.0777270660795228,-0.025130549630523812,-0.029875145372961712,0.019255216717706494,0.013117203459757526,-0.07160050556033871,0.044524972286105136,-0.056780038227199923,-0.007080972116158972,-0.0016841881520650891,0.009087646986509478,0.026151567096497085,0.021114954326253082,-0.1269340146902196,-0.005676510123479137,0.05398663980778047,0.0026529734808041954,0.01208363535210252,0.0318425986756771,-4.406660469654991E-4,-0.0038469612954941113,-0.03372569480060795,-0.09066216464067788,0.007190218382632782,-0.03312360850446397,0.027211981990677845,0.02483781263215987,0.01630497156834598,-0.047580174914274236,0.01090316762362585,0.02743201835289446,0.0017425844433641624,-0.03580083113673365,0.0067454096597218665,-0.02806762962427456,0.021011611901908613,0.0977711274612121,0.05840684828953547,-0.027726174517653794,0.04413483504829983,-0.02351571827941455,-0.03264092489214733,-0.0543707497177651,0.025159678338664084,-9.86891551630918E-4,-0.029496228709873788,0.016395744569139382,0.13333857845316024,0.11650330971975183,0.030600413567722275,-0.032125101438459536,-0.013714076044527965,0.03599450626110368,-0.028204840349052424,0.04580524806061835,-0.039912409137096524,-0.11205664575763241,-0.05637828975869884,0.16036292395353463,-0.03346267257201848].\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.classification.NaiveBayes$.requireNonnegativeValues(NaiveBayes.scala:235)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$4.apply(NaiveBayes.scala:144)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$4.apply(NaiveBayes.scala:144)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$7.apply(NaiveBayes.scala:168)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$7.apply(NaiveBayes.scala:166)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$aggregateByKey$1$$anonfun$apply$6.apply(PairRDDFunctions.scala:172)\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:189)\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:188)\n\tat org.apache.spark.util.collection.AppendOnlyMap.changeValue(AppendOnlyMap.scala:144)\n\tat org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(SizeTrackingAppendOnlyMap.scala:32)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:194)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1.apply(NaiveBayes.scala:176)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1.apply(NaiveBayes.scala:129)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:183)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:183)\n\tat org.apache.spark.ml.classification.NaiveBayes.trainWithLabelCheck(NaiveBayes.scala:129)\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:118)\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:78)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Naive Bayes requires nonnegative feature values but found [-0.0813618281004723,0.032609838120934424,-0.03778935440522917,-0.10364296923826048,0.05866389085205683,0.043506006094651446,-0.020035373965456953,6.58658283290881E-4,-0.0027596470361797485,-0.036454196965530365,0.03091503046693275,-0.024912650818551745,0.05985457125237634,0.013168900686270166,0.07241504154842195,-0.06052869467266434,-0.012242375916177353,-0.06086302072968507,0.10797833527469139,0.05335005043584821,0.1044013091159286,0.10547053876575245,-0.05264355177031004,-0.029077896800773422,0.10194337404492787,-0.04681117238807167,-0.08145223454122844,-0.027785475441538268,-0.034363409402769406,0.014576943840650126,0.02864424111375715,0.023761392692862034,-0.028002761636703642,0.0017686769311948474,0.05585292022628574,-0.10172763996524736,0.02417167796087343,0.07091075816466866,-0.030473114661723484,0.05108258736165522,-0.0777270660795228,-0.025130549630523812,-0.029875145372961712,0.019255216717706494,0.013117203459757526,-0.07160050556033871,0.044524972286105136,-0.056780038227199923,-0.007080972116158972,-0.0016841881520650891,0.009087646986509478,0.026151567096497085,0.021114954326253082,-0.1269340146902196,-0.005676510123479137,0.05398663980778047,0.0026529734808041954,0.01208363535210252,0.0318425986756771,-4.406660469654991E-4,-0.0038469612954941113,-0.03372569480060795,-0.09066216464067788,0.007190218382632782,-0.03312360850446397,0.027211981990677845,0.02483781263215987,0.01630497156834598,-0.047580174914274236,0.01090316762362585,0.02743201835289446,0.0017425844433641624,-0.03580083113673365,0.0067454096597218665,-0.02806762962427456,0.021011611901908613,0.0977711274612121,0.05840684828953547,-0.027726174517653794,0.04413483504829983,-0.02351571827941455,-0.03264092489214733,-0.0543707497177651,0.025159678338664084,-9.86891551630918E-4,-0.029496228709873788,0.016395744569139382,0.13333857845316024,0.11650330971975183,0.030600413567722275,-0.032125101438459536,-0.013714076044527965,0.03599450626110368,-0.028204840349052424,0.04580524806061835,-0.039912409137096524,-0.11205664575763241,-0.05637828975869884,0.16036292395353463,-0.03346267257201848].\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.classification.NaiveBayes$.requireNonnegativeValues(NaiveBayes.scala:235)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$4.apply(NaiveBayes.scala:144)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$4.apply(NaiveBayes.scala:144)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$7.apply(NaiveBayes.scala:168)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$7.apply(NaiveBayes.scala:166)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$aggregateByKey$1$$anonfun$apply$6.apply(PairRDDFunctions.scala:172)\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:189)\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:188)\n\tat org.apache.spark.util.collection.AppendOnlyMap.changeValue(AppendOnlyMap.scala:144)\n\tat org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(SizeTrackingAppendOnlyMap.scala:32)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:194)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# Chain labelIndexer, vecAssembler and NBmodel in a \n",
    "pipeline_w2v = Pipeline(stages=[labelIndexer, vecAssembler, nb])\n",
    "\n",
    "# Run stages in pipeline and train model\n",
    "model_w2v = pipeline_w2v.fit(trainingData_w2v)\n",
    "\n",
    "# Make predictions on testData so we can measure the accuracy of our model on new data\n",
    "predictions_w2v = model_w2v.transform(testData_w2v)\n",
    "\n",
    "# Evaluation of model\n",
    "accuracy = evaluator.evaluate(predictions_w2v)\n",
    "print(\"Model Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T03:47:40.709158Z",
     "start_time": "2019-10-17T03:47:38.862318Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a Random Forest model\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n",
    "\n",
    "# Chain labelIndexer, vecAssembler and NBmodel in a \n",
    "pipeline_w2v = Pipeline(stages=[labelIndexer, vecAssembler, rf])\n",
    "\n",
    "# Run stages in pipeline and train model\n",
    "model_w2v = pipeline_w2v.fit(trainingData_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T03:47:46.368240Z",
     "start_time": "2019-10-17T03:47:46.223192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- req_guid: string (nullable = true)\n",
      " |-- job_description_vectors: vector (nullable = true)\n",
      " |-- labeled: string (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on testData so we can measure the accuracy of our model on new data\n",
    "predictions_w2v = model_w2v.transform(testData_w2v)\n",
    "\n",
    "# Display what results we can view\n",
    "predictions_w2v.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T03:18:31.258472Z",
     "start_time": "2019-10-17T03:18:31.169772Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+\n",
      "|label|prediction|         probability|\n",
      "+-----+----------+--------------------+\n",
      "|  0.0|       0.0|[0.98814832182249...|\n",
      "|  0.0|       0.0|[0.98982595976289...|\n",
      "|  0.0|       0.0|[0.96681039654436...|\n",
      "|  0.0|       0.0|[0.98688188596183...|\n",
      "|  0.0|       0.0|[0.98729374269562...|\n",
      "|  0.0|       0.0|[0.98932703768097...|\n",
      "|  0.0|       0.0|[0.98814832182249...|\n",
      "|  0.0|       0.0|[0.98551264930407...|\n",
      "|  5.0|       0.0|[0.98537961893635...|\n",
      "|  0.0|       0.0|[0.98559518282344...|\n",
      "|  0.0|       0.0|[0.98982595976289...|\n",
      "|  0.0|       0.0|[0.98473875433743...|\n",
      "|  0.0|       0.0|[0.9838966303691,...|\n",
      "|  0.0|       0.0|[0.98814832182249...|\n",
      "|  0.0|       0.0|[0.98855952390223...|\n",
      "|  1.0|       0.0|[0.50985258449615...|\n",
      "|  0.0|       0.0|[0.97906190685285...|\n",
      "|  0.0|       0.0|[0.87148980284445...|\n",
      "|  0.0|       0.0|[0.98814832182249...|\n",
      "|  1.0|       1.0|[0.17753193366288...|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_w2v.select(\"label\", \"prediction\", \"probability\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T03:48:10.442584Z",
     "start_time": "2019-10-17T03:48:10.268716Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mModel Accuracy:  0.9680129240710824\n"
     ]
    }
   ],
   "source": [
    "# Model evluation\n",
    "accuracy = evaluator.evaluate(predictions_w2v)\n",
    "print(bcolors.BOLD + \"Model Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation\n",
    "Experimenting with Various numTrees Parameters, \n",
    "I can experiment with various smoothing parameters to see which returns the best result. \n",
    "This is easily done with the ParamGridBuilder and CrossValidator.\n",
    "As indicated as 6 values for the smoothing parameter, this grid will provide 6 parameter settings \n",
    "for CrossValidator to model, evaluate and choose from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T03:20:41.446197Z",
     "start_time": "2019-10-17T03:20:41.433512Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create ParamGrid and Evaluator for Cross Validation\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [5, 10, 15, 20, 25, 30]).build()\n",
    "cvEvaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T03:28:43.028495Z",
     "start_time": "2019-10-17T03:28:42.829592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7448\n",
      "3095\n"
     ]
    }
   ],
   "source": [
    "# Split dataset randomly into Training and Test sets. Set seed for reproducibility\n",
    "(trainingData_w2v, testData_w2v) = result_w2v.randomSplit([0.7, 0.3], seed = 100)\n",
    "\n",
    "trainingData_w2v.cache()\n",
    "testData_w2v.cache()\n",
    "\n",
    "print(trainingData_w2v.count())\n",
    "print(testData_w2v.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T03:31:54.341975Z",
     "start_time": "2019-10-17T03:31:54.322927Z"
    }
   },
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"labeled\", outputCol=\"label\")\n",
    "vecAssembler = VectorAssembler(inputCols=[\"job_description_vectors\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T03:48:44.464349Z",
     "start_time": "2019-10-17T03:48:42.191227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m\u001b[1mModel Accuracy:  0.9683360258481422\n"
     ]
    }
   ],
   "source": [
    "# Train a Random Forest model with numTrees = 15\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=15)\n",
    "\n",
    "# Chain labelIndexer, vecAssembler and NBmodel in a \n",
    "pipeline_w2v = Pipeline(stages=[labelIndexer, vecAssembler, rf])\n",
    "\n",
    "# Run stages in pipeline and train model\n",
    "model_w2v = pipeline_w2v.fit(trainingData_w2v)\n",
    "\n",
    "# Make predictions on testData so we can measure the accuracy of our model on new data\n",
    "predictions_w2v = model_w2v.transform(testData_w2v)\n",
    "\n",
    "# Evaluation of model\n",
    "accuracy = evaluator.evaluate(predictions_w2v)\n",
    "print(bcolors.OKBLUE + bcolors.BOLD +\"Model Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T03:48:54.391344Z",
     "start_time": "2019-10-17T03:48:52.067241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m\u001b[1mModel Accuracy:  0.9686591276252019\n"
     ]
    }
   ],
   "source": [
    "# Train a Random Forest model with numTrees = 20\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=20)\n",
    "\n",
    "# Chain labelIndexer, vecAssembler and NBmodel in a \n",
    "pipeline_w2v = Pipeline(stages=[labelIndexer, vecAssembler, rf])\n",
    "\n",
    "# Run stages in pipeline and train model\n",
    "model_w2v = pipeline_w2v.fit(trainingData_w2v)\n",
    "\n",
    "# Make predictions on testData so we can measure the accuracy of our model on new data\n",
    "predictions_w2v = model_w2v.transform(testData_w2v)\n",
    "\n",
    "# Evaluation of model\n",
    "accuracy = evaluator.evaluate(predictions_w2v)\n",
    "print(bcolors.OKBLUE + bcolors.BOLD +\"Model Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T03:49:08.471577Z",
     "start_time": "2019-10-17T03:49:05.763813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m\u001b[1mModel Accuracy:  0.9676898222940227\n"
     ]
    }
   ],
   "source": [
    "# Train a Random Forest model with numTrees = 30\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=30)\n",
    "\n",
    "# Chain labelIndexer, vecAssembler and NBmodel in a \n",
    "pipeline_w2v = Pipeline(stages=[labelIndexer, vecAssembler, rf])\n",
    "\n",
    "# Run stages in pipeline and train model\n",
    "model_w2v = pipeline_w2v.fit(trainingData_w2v)\n",
    "\n",
    "# Make predictions on testData so we can measure the accuracy of our model on new data\n",
    "predictions_w2v = model_w2v.transform(testData_w2v)\n",
    "\n",
    "# Evaluation of model\n",
    "accuracy = evaluator.evaluate(predictions_w2v)\n",
    "print(bcolors.OKBLUE + bcolors.BOLD +\"Model Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T03:17:51.395467Z",
     "start_time": "2019-10-17T03:17:50.269140Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train a Random Forest model\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n",
    "\n",
    "# Chain labelIndexer, vecAssembler and NBmodel in a \n",
    "pipeline_tfidf = Pipeline(stages=[labelIndexer, vecAssembler, rf])\n",
    "\n",
    "# Run stages in pipeline and train model\n",
    "model_tfidf = pipeline_tfidf.fit(trainingData_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T03:18:34.821114Z",
     "start_time": "2019-10-17T03:18:34.714075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- req_guid: string (nullable = true)\n",
      " |-- job_description: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- labeled: string (nullable = true)\n",
      " |-- TFs: vector (nullable = true)\n",
      " |-- job_description_vectors: vector (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on testData so we can measure the accuracy of our model on new data\n",
    "predictions_tfidf = model_tfidf.transform(testData_tfidf)\n",
    "\n",
    "# Display what results we can view\n",
    "predictions_tfidf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T03:18:38.068819Z",
     "start_time": "2019-10-17T03:18:37.992068Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+\n",
      "|label|prediction|         probability|\n",
      "+-----+----------+--------------------+\n",
      "|  0.0|       0.0|[0.96409210366217...|\n",
      "|  0.0|       0.0|[0.97142433402069...|\n",
      "|  0.0|       0.0|[0.94718382047853...|\n",
      "|  0.0|       0.0|[0.96283012809498...|\n",
      "|  0.0|       0.0|[0.93419815298044...|\n",
      "|  0.0|       0.0|[0.94414371698564...|\n",
      "|  0.0|       0.0|[0.93442381621952...|\n",
      "|  0.0|       0.0|[0.93236961262148...|\n",
      "|  5.0|       0.0|[0.94331817248805...|\n",
      "|  0.0|       0.0|[0.97142433402069...|\n",
      "|  0.0|       0.0|[0.96732115393027...|\n",
      "|  0.0|       0.0|[0.88830194339645...|\n",
      "|  0.0|       0.0|[0.88001362123660...|\n",
      "|  0.0|       0.0|[0.93559866288958...|\n",
      "|  0.0|       0.0|[0.92198164854127...|\n",
      "|  1.0|       0.0|[0.64232095659484...|\n",
      "|  0.0|       0.0|[0.94058927872568...|\n",
      "|  0.0|       0.0|[0.90947466502509...|\n",
      "|  0.0|       0.0|[0.95881393133862...|\n",
      "|  1.0|       0.0|[0.87738093704292...|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_tfidf.select(\"label\", \"prediction\", \"probability\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T03:05:36.529633Z",
     "start_time": "2019-10-17T03:05:36.283229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy:  0.9231017770597738\n"
     ]
    }
   ],
   "source": [
    "# Model evluation\n",
    "accuracy = evaluator.evaluate(predictions_tfidf)\n",
    "print(\"Model Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T03:18:14.805707Z",
     "start_time": "2019-10-17T03:18:13.728544Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train a Random Forest model\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n",
    "\n",
    "# Chain labelIndexer, vecAssembler and NBmodel in a \n",
    "pipeline_tf = Pipeline(stages=[labelIndexer, vecAssembler, rf])\n",
    "\n",
    "# Run stages in pipeline and train model\n",
    "model_tf = pipeline_tf.fit(trainingData_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T03:18:41.667904Z",
     "start_time": "2019-10-17T03:18:41.595574Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- req_guid: string (nullable = true)\n",
      " |-- job_description: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- labeled: string (nullable = true)\n",
      " |-- job_description_vectors: vector (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on testData so we can measure the accuracy of our model on new data\n",
    "predictions_tf = model_tf.transform(testData_tf)\n",
    "\n",
    "# Display what results we can view\n",
    "predictions_tf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T03:18:45.157692Z",
     "start_time": "2019-10-17T03:18:45.062582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+\n",
      "|label|prediction|         probability|\n",
      "+-----+----------+--------------------+\n",
      "|  0.0|       0.0|[0.96409210366217...|\n",
      "|  0.0|       0.0|[0.97142433402069...|\n",
      "|  0.0|       0.0|[0.94718382047853...|\n",
      "|  0.0|       0.0|[0.96283012809498...|\n",
      "|  0.0|       0.0|[0.93419815298044...|\n",
      "|  0.0|       0.0|[0.94414371698564...|\n",
      "|  0.0|       0.0|[0.93442381621952...|\n",
      "|  0.0|       0.0|[0.93236961262148...|\n",
      "|  5.0|       0.0|[0.94331817248805...|\n",
      "|  0.0|       0.0|[0.97142433402069...|\n",
      "|  0.0|       0.0|[0.96732115393027...|\n",
      "|  0.0|       0.0|[0.88830194339645...|\n",
      "|  0.0|       0.0|[0.88001362123660...|\n",
      "|  0.0|       0.0|[0.93559866288958...|\n",
      "|  0.0|       0.0|[0.92198164854127...|\n",
      "|  1.0|       0.0|[0.64232095659484...|\n",
      "|  0.0|       0.0|[0.94058927872568...|\n",
      "|  0.0|       0.0|[0.90947466502509...|\n",
      "|  0.0|       0.0|[0.95881393133862...|\n",
      "|  1.0|       0.0|[0.87738093704292...|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_tf.select(\"label\", \"prediction\", \"probability\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T03:06:03.854616Z",
     "start_time": "2019-10-17T03:06:03.574207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy:  0.9231017770597738\n"
     ]
    }
   ],
   "source": [
    "# Model evluation\n",
    "accuracy = evaluator.evaluate(predictions_tf)\n",
    "print(\"Model Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Dense Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T19:17:40.830456Z",
     "start_time": "2019-10-17T19:17:40.819612Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define layers for the neural network:\n",
    "# input layer of size 100 (feature vectors), \n",
    "# two intermediate of size 10 and 5\n",
    "# and output of size 10 (classes)\n",
    "layers = [100, 10, 5, 10]\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "trainer = MultilayerPerceptronClassifier(maxIter=1000, layers=layers, blockSize=128, seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "Here, I am going to only test word2vec with neural network since neural network won't work well with TF or TFIDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T19:18:23.205968Z",
     "start_time": "2019-10-17T19:17:43.341928Z"
    }
   },
   "outputs": [],
   "source": [
    "# Chain labelIndexer, vecAssembler and NN in a \n",
    "pipeline_w2v = Pipeline(stages=[labelIndexer, vecAssembler, trainer])\n",
    "\n",
    "# Run stages in pipeline and train model\n",
    "model_w2v = pipeline_w2v.fit(trainingData_w2v)\n",
    "\n",
    "# Make predictions on testData so we can measure the accuracy of our model on new data\n",
    "predictions_w2v = model_w2v.transform(testData_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T19:18:42.504000Z",
     "start_time": "2019-10-17T19:18:42.347548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mModel Accuracy:  0.9825525040387723\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of model\n",
    "accuracy = evaluator.evaluate(predictions_w2v)\n",
    "print(bcolors.BOLD + \"Model Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this notebook I went through major steps to demonstrate and compare different technologies to solve NLP a classification problem. During this project implementation, I used Spark SQL and ML libraries running on a local machine.\n",
    "\n",
    "From the results, one can easily see that combining neural network and word2vec together is a really powerful approach to solve NLP problems.\n",
    "\n",
    "However, unfortunately Spark only provides word2vec and neural network (dense) multi-class classifier available. If someone wants to try doc2vec or sentence2vec or neural network regression, gensim and scikit-learn libraries can be an option. Of course, one can also take a look at Tensorflow and construct embedding and dense neural network layers to solve the same NLP problems."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
