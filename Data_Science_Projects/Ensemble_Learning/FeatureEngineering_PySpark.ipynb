{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T15:14:10.501895Z",
     "start_time": "2021-01-22T15:14:10.495241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.0.1\n"
     ]
    }
   ],
   "source": [
    "# This cell is not needed when this jupyter notebook is running on a Sagemaker instance\n",
    "# This is only needed when running it on local laptop\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "# Set up a spark session with leveraging all available CPUs\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master('local[*]')\\\n",
    "        .appName(\"Demo\") \\\n",
    "        .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "        .getOrCreate()\n",
    "print(\"Spark Version: \" + spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T21:53:19.651813Z",
     "start_time": "2021-01-22T21:53:19.648409Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, mean\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T20:02:13.260384Z",
     "start_time": "2021-01-22T20:02:13.168270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|Color|\n",
      "+---+-----+\n",
      "|  0|  Red|\n",
      "|  1| Blue|\n",
      "|  2|Green|\n",
      "|  3| null|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a data frame\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"Red\"),\n",
    "    (1, \"Blue\"),\n",
    "    (2, \"Green\"),\n",
    "    (3, None)\n",
    "], [\"id\", \"Color\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop rows with NULL values in ANY columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T20:02:17.443962Z",
     "start_time": "2021-01-22T20:02:17.352795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|id |Color|\n",
      "+---+-----+\n",
      "|0  |Red  |\n",
      "|1  |Blue |\n",
      "|2  |Green|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(\"any\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop rows with NULL values in ALL columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T20:03:25.502304Z",
     "start_time": "2021-01-22T20:03:25.370245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|id |Color|\n",
      "+---+-----+\n",
      "|0  |Red  |\n",
      "|1  |Blue |\n",
      "|2  |Green|\n",
      "|3  |null |\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(\"all\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop rows with NULL values by using ```dropna()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T20:02:40.686821Z",
     "start_time": "2021-01-22T20:02:40.577076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|id |Color|\n",
      "+---+-----+\n",
      "|0  |Red  |\n",
      "|1  |Blue |\n",
      "|2  |Green|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropna().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill the NULL Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T20:19:36.929411Z",
     "start_time": "2021-01-22T20:19:36.794162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|Color|\n",
      "+---+-----+\n",
      "|  0|  Red|\n",
      "|  1| Blue|\n",
      "|  2|Green|\n",
      "|  3| null|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a data frame\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"Red\"),\n",
    "    (1, \"Blue\"),\n",
    "    (2, \"Green\"),\n",
    "    (3, None)\n",
    "], [\"id\", \"Color\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the null values with \"NA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T20:20:11.298038Z",
     "start_time": "2021-01-22T20:20:11.186735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|Color|\n",
      "+---+-----+\n",
      "|  0|  Red|\n",
      "|  1| Blue|\n",
      "|  2|Green|\n",
      "|  3|   NA|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill('NA').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the null values with mean or average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T20:21:56.170271Z",
     "start_time": "2021-01-22T20:21:56.051737Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|  id| Color|\n",
      "+----+------+\n",
      "|   0|   Red|\n",
      "|   1|  Blue|\n",
      "|   2| Green|\n",
      "|null|Yellow|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (0, \"Red\"),\n",
    "    (1, \"Blue\"),\n",
    "    (2, \"Green\"),\n",
    "    (None, \"Yellow\")\n",
    "], [\"id\", \"Color\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T20:25:12.218741Z",
     "start_time": "2021-01-22T20:25:11.921282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id| Color|\n",
      "+---+------+\n",
      "|  0|   Red|\n",
      "|  1|  Blue|\n",
      "|  2| Green|\n",
      "|  1|Yellow|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mean_val=df.select(mean(df.id)).collect()\n",
    "df.na.fill(mean_val[0][0],subset=['id']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T15:16:55.746030Z",
     "start_time": "2021-01-22T15:16:55.560590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|Color|\n",
      "+---+-----+\n",
      "|  0|  Red|\n",
      "|  1| Blue|\n",
      "|  2|Green|\n",
      "|  3|White|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a data frame\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"Red\"),\n",
    "    (1, \"Blue\"),\n",
    "    (2, \"Green\"),\n",
    "    (3, \"White\")\n",
    "], [\"id\", \"Color\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T15:16:58.971521Z",
     "start_time": "2021-01-22T15:16:57.761693Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+-------------------+\n",
      "|id |Color|Color_Array|Color_OneHotEncoded|\n",
      "+---+-----+-----------+-------------------+\n",
      "|0  |Red  |[Red]      |(4,[3],[1.0])      |\n",
      "|1  |Blue |[Blue]     |(4,[1],[1.0])      |\n",
      "|2  |Green|[Green]    |(4,[0],[1.0])      |\n",
      "|3  |White|[White]    |(4,[2],[1.0])      |\n",
      "+---+-----+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding with Pyspark CountVectorizer\n",
    "\n",
    "df = df.withColumn(\"Color_Array\", split(col(\"Color\"),\" \"))\n",
    "colorVectorizer = CountVectorizer(inputCol=\"Color_Array\", outputCol=\"Color_OneHotEncoded\", vocabSize=4, minDF=1.0)\n",
    "colorVectorizer_model = colorVectorizer.fit(df)\n",
    "df_ohe = colorVectorizer_model.transform(df)\n",
    "df_ohe.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T15:21:16.546946Z",
     "start_time": "2021-01-22T15:21:16.454905Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the one-hot encoded column into numpy array\n",
    "\n",
    "x_3d = np.array(df_ohe.select('Color_OneHotEncoded').collect())\n",
    "X = x_3d.reshape(4, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Index Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T15:26:54.079182Z",
     "start_time": "2021-01-22T15:26:53.917001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|Color|\n",
      "+---+-----+\n",
      "|  0|  Red|\n",
      "|  1| Blue|\n",
      "|  2|Green|\n",
      "|  3|White|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a data frame\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"Red\"),\n",
    "    (1, \"Blue\"),\n",
    "    (2, \"Green\"),\n",
    "    (3, \"White\")\n",
    "], [\"id\", \"Color\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T19:00:14.541421Z",
     "start_time": "2021-01-22T19:00:13.579358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+\n",
      "|id |Color|Color_Index|\n",
      "+---+-----+-----------+\n",
      "|0  |Red  |2.0        |\n",
      "|1  |Blue |0.0        |\n",
      "|2  |Green|1.0        |\n",
      "|3  |White|3.0        |\n",
      "+---+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert a column into numerical categories (indexing)\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"Color\", outputCol=\"Color_Index\")\n",
    "labelIndexer_model = labelIndexer.fit(df)\n",
    "df_lie = labelIndexer_model.transform(df)\n",
    "df_lie.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T19:14:57.757856Z",
     "start_time": "2021-01-22T19:14:57.552149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+\n",
      "| id|Color|Color_Array|\n",
      "+---+-----+-----------+\n",
      "|  0|  Red|      [Red]|\n",
      "|  1| Blue|     [Blue]|\n",
      "|  2|Green|    [Green]|\n",
      "|  3|White|    [White]|\n",
      "+---+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a data frame\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"Red\"),\n",
    "    (1, \"Blue\"),\n",
    "    (2, \"Green\"),\n",
    "    (3, \"White\")\n",
    "], [\"id\", \"Color\"])\n",
    "df = df.withColumn(\"Color_Array\", split(col(\"Color\"),\" \"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T19:22:09.022236Z",
     "start_time": "2021-01-22T19:22:08.546492Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup one-hot encoding\n",
    "\n",
    "colorVectorizer = CountVectorizer(inputCol=\"Color_Array\", outputCol=\"Color_OneHotEncoded\", vocabSize=4, minDF=1.0)\n",
    "colorVectorizer_model = colorVectorizer.fit(df)\n",
    "df_ohe = colorVectorizer_model.transform(df)\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"Color\", outputCol=\"Color_Index\")\n",
    "labelIndexer_model = labelIndexer.fit(df_ohe)\n",
    "df_lie = labelIndexer_model.transform(df_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T19:22:45.825956Z",
     "start_time": "2021-01-22T19:22:45.674746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+-------------------+-----------+-------------------+\n",
      "|id |Color|Color_Array|Color_OneHotEncoded|Color_Index|features           |\n",
      "+---+-----+-----------+-------------------+-----------+-------------------+\n",
      "|0  |Red  |[Red]      |(4,[0],[1.0])      |2.0        |(5,[0,4],[1.0,2.0])|\n",
      "|1  |Blue |[Blue]     |(4,[3],[1.0])      |0.0        |(5,[3],[1.0])      |\n",
      "|2  |Green|[Green]    |(4,[2],[1.0])      |1.0        |(5,[2,4],[1.0,1.0])|\n",
      "|3  |White|[White]    |(4,[1],[1.0])      |3.0        |(5,[1,4],[1.0,3.0])|\n",
      "+---+-----+-----------+-------------------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vecAssembler = VectorAssembler(inputCols=[\"Color_OneHotEncoded\", \"Color_Index\"], outputCol=\"features\")\n",
    "df_va = vecAssembler.transform(df_lie)\n",
    "df_va.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T22:01:23.170744Z",
     "start_time": "2021-01-22T22:01:23.047752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Factor|Color|\n",
      "+------+-----+\n",
      "|     0| 10.0|\n",
      "|     1| 11.0|\n",
      "|     2| 12.0|\n",
      "|     3| 13.0|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a data frame\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0, 10.0),\n",
    "    (1, 11.0),\n",
    "    (2, 12.0),\n",
    "    (3, 13.0)\n",
    "], [\"Factor\", \"Color\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T22:03:43.229707Z",
     "start_time": "2021-01-22T22:03:43.024673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+-----------------------------------------+\n",
      "|Factor|Color|Color_VA  |scaledColor                              |\n",
      "+------+-----+----------+-----------------------------------------+\n",
      "|0     |10.0 |[0.0,10.0]|[-1.161895003862225,-1.161895003862225]  |\n",
      "|1     |11.0 |[1.0,11.0]|[-0.3872983346207417,-0.3872983346207417]|\n",
      "|2     |12.0 |[2.0,12.0]|[0.3872983346207417,0.3872983346207417]  |\n",
      "|3     |13.0 |[3.0,13.0]|[1.161895003862225,1.161895003862225]    |\n",
      "+------+-----+----------+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "va = VectorAssembler(inputCols=[\"Factor\",\"Color\"], outputCol=\"Color_VA\")\n",
    "df_tmp = va.transform(df)\n",
    "scaler = StandardScaler(inputCol=\"Color_VA\", outputCol=\"scaledColor\", withStd=True, withMean=True)\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(df_tmp)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "scaledData = scalerModel.transform(df_tmp)\n",
    "scaledData.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
