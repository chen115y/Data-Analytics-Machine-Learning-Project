{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction:-Word2Vec-Training-for-Job-Descriptions\" data-toc-modified-id=\"Introduction:-Word2Vec-Training-for-Job-Descriptions-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction: Word2Vec Training for Job Descriptions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dataset\" data-toc-modified-id=\"Dataset-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Dataset</a></span></li><li><span><a href=\"#Python-Library\" data-toc-modified-id=\"Python-Library-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Python Library</a></span></li></ul></li><li><span><a href=\"#Data-Set-Loading-and-Cleaning-Up\" data-toc-modified-id=\"Data-Set-Loading-and-Cleaning-Up-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Set Loading and Cleaning Up</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-Job-Description-CSV-Data\" data-toc-modified-id=\"Load-Job-Description-CSV-Data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Load Job Description CSV Data</a></span></li><li><span><a href=\"#Clean-Up\" data-toc-modified-id=\"Clean-Up-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Clean Up</a></span></li></ul></li><li><span><a href=\"#Word-Embedding-Training-with-Word2Vec\" data-toc-modified-id=\"Word-Embedding-Training-with-Word2Vec-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Word Embedding Training with Word2Vec</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tokenization\" data-toc-modified-id=\"Tokenization-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Tokenization</a></span></li><li><span><a href=\"#Lemmatization\" data-toc-modified-id=\"Lemmatization-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Lemmatization</a></span></li><li><span><a href=\"#Word-Embedding-Training-with-Gensim\" data-toc-modified-id=\"Word-Embedding-Training-with-Gensim-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Word Embedding Training with Gensim</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Word2Vec Training for Job Descriptions\n",
    "\n",
    "In this notebook, I am going to train word2vec word embedding with ~2.5 million job descriptions (from Cloudera production rf_job_description table). I will use NLTK and gensim word2vec to do the training and save the model as a binary file.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset is a historical data of job descriptions stored as \"JobDescriptions_2.5M.csv\" file.\n",
    "\n",
    "## Python Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T18:43:48.862170Z",
     "start_time": "2020-01-28T18:43:45.160230Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pandas and numpy for converting from Spark dataframe into Pandas dataframe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Make the random numbers predictable\n",
    "np.random.seed(42)\n",
    "import multiprocessing\n",
    "cpu_count = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T18:43:48.867440Z",
     "start_time": "2020-01-28T18:43:48.864535Z"
    }
   },
   "outputs": [],
   "source": [
    "# Allow multiple output/display from one cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T18:43:50.448641Z",
     "start_time": "2020-01-28T18:43:48.869923Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "    \n",
    "# Stop Word Removal\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T18:43:50.470635Z",
     "start_time": "2020-01-28T18:43:50.451012Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T18:43:50.678172Z",
     "start_time": "2020-01-28T18:43:50.472875Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ivan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ivan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw to /home/ivan/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ivan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set Loading and Cleaning Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Job Description CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T18:44:15.121275Z",
     "start_time": "2020-01-28T18:43:50.680554Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2452237 entries, 0 to 2452236\n",
      "Data columns (total 3 columns):\n",
      "req_guid           object\n",
      "job_title          object\n",
      "job_description    object\n",
      "dtypes: object(3)\n",
      "memory usage: 56.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../JobDescriptions_2.5M.csv', header='infer')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T18:44:15.126359Z",
     "start_time": "2020-01-28T18:44:15.123198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total amount of training data on job descriptions is:  2452237\n"
     ]
    }
   ],
   "source": [
    "print(\"The total amount of training data on job descriptions is: \", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T18:44:16.018075Z",
     "start_time": "2020-01-28T18:44:15.129408Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove any rows without job_description\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T18:44:21.374766Z",
     "start_time": "2020-01-28T18:44:16.021272Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combine simply job description and title so that job title is a part of job description\n",
    "df['job_description'] = df['job_title'] + \" \" + df['job_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T18:44:21.451116Z",
     "start_time": "2020-01-28T18:44:21.436415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2307199 entries, 0 to 2452236\n",
      "Data columns (total 3 columns):\n",
      "req_guid           object\n",
      "job_title          object\n",
      "job_description    object\n",
      "dtypes: object(3)\n",
      "memory usage: 70.4+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>req_guid</th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15983255</td>\n",
       "      <td>Handler</td>\n",
       "      <td>Handler Ability to lift 75 lbs Ability to mane...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14382131</td>\n",
       "      <td>Receiver day shift</td>\n",
       "      <td>Receiver day shift  Contract role may be taken...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15255153</td>\n",
       "      <td>NET Developer PCI Compliance</td>\n",
       "      <td>NET Developer PCI Compliance Our client is lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HPJP00081328</td>\n",
       "      <td>US Business Analyst 3</td>\n",
       "      <td>US Business Analyst 3 TEC to Genuent Transfer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SSBK26782-1</td>\n",
       "      <td>Team Member 3 Years</td>\n",
       "      <td>Team Member 3 Years   Job Description TBD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       req_guid                      job_title  \\\n",
       "0      15983255                        Handler   \n",
       "1      14382131            Receiver day shift    \n",
       "2      15255153   NET Developer PCI Compliance   \n",
       "3  HPJP00081328          US Business Analyst 3   \n",
       "4   SSBK26782-1           Team Member 3 Years    \n",
       "\n",
       "                                     job_description  \n",
       "0  Handler Ability to lift 75 lbs Ability to mane...  \n",
       "1  Receiver day shift  Contract role may be taken...  \n",
       "2   NET Developer PCI Compliance Our client is lo...  \n",
       "3  US Business Analyst 3 TEC to Genuent Transfer ...  \n",
       "4         Team Member 3 Years   Job Description TBD   "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding Training with Word2Vec\n",
    "\n",
    "In this step, raw text data will be transformed into feature vectors. I will implement the following steps in order to obtain relevant features from the dataset.\n",
    "\n",
    "* Tokenizing\n",
    "* Remove stop words\n",
    "* Lemmatization (not stem since stemming can reduce the interpretability) \n",
    "* Word Embeddings Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization is the process by dividing the quantity of text into smaller parts called tokens so that each token can be further treated for machine learning purposes. A token can be a character, a word, a sentence or a paragraph. In this notebook, I only consider words as tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T18:46:54.593405Z",
     "start_time": "2020-01-28T18:44:21.453465Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize the job description and title\n",
    "# I can use NLTK word_tokenize function to process the job description field (by removing punctuations \n",
    "# and separating words) like below\n",
    "# df['job_description'] = df.apply(lambda row: word_tokenize(row.job_description), axis=1)\n",
    "# Or I can just use python string split function to separate text since the job description has been cleaned\n",
    "df['job_description'] = df[\"job_description\"].str.lower()\n",
    "df['job_description'] = df[\"job_description\"].str.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T18:46:54.613875Z",
     "start_time": "2020-01-28T18:46:54.598382Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>req_guid</th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15983255</td>\n",
       "      <td>Handler</td>\n",
       "      <td>[handler, ability, to, lift, 75, lbs, ability,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14382131</td>\n",
       "      <td>Receiver day shift</td>\n",
       "      <td>[receiver, day, shift, , contract, role, may, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15255153</td>\n",
       "      <td>NET Developer PCI Compliance</td>\n",
       "      <td>[, net, developer, pci, compliance, our, clien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HPJP00081328</td>\n",
       "      <td>US Business Analyst 3</td>\n",
       "      <td>[us, business, analyst, 3, tec, to, genuent, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SSBK26782-1</td>\n",
       "      <td>Team Member 3 Years</td>\n",
       "      <td>[team, member, 3, years, , , job, description,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       req_guid                      job_title  \\\n",
       "0      15983255                        Handler   \n",
       "1      14382131            Receiver day shift    \n",
       "2      15255153   NET Developer PCI Compliance   \n",
       "3  HPJP00081328          US Business Analyst 3   \n",
       "4   SSBK26782-1           Team Member 3 Years    \n",
       "\n",
       "                                     job_description  \n",
       "0  [handler, ability, to, lift, 75, lbs, ability,...  \n",
       "1  [receiver, day, shift, , contract, role, may, ...  \n",
       "2  [, net, developer, pci, compliance, our, clien...  \n",
       "3  [us, business, analyst, 3, tec, to, genuent, t...  \n",
       "4  [team, member, 3, years, , , job, description,...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Stopword Removal__ (not used)\n",
    "\n",
    "A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a NLP program has been programmed to ignore. In this notebook, I will use NLTK stop words dataset to remove any stop words in job description field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T18:46:54.629149Z",
     "start_time": "2020-01-28T18:46:54.616168Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Get stopwords list from NLTK library\n",
    "# stop_words = stopwords.words('english')\n",
    "# # Define a function to remove any stop words from input text\n",
    "# def removeStopWords(x):\n",
    "#         return [w.lower() for w in x if (w not in stop_words) and (w != '') and (w is not None)]\n",
    "# # Apply the defined function to remove stop words for job descriptions\n",
    "# df['job_description'] = df.apply(lambda row: removeStopWords(row.job_description), axis=1)\n",
    "# # Show some results\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatisation (or lemmatization) in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form. For example, in English, the verb 'to walk' may appear as 'walk', 'walked', 'walks', 'walking'. The base form, 'walk', that one might look up in a dictionary, is called the lemma for the word. I will use NLTK lemmatization function to convert words into their lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T19:25:57.668761Z",
     "start_time": "2020-01-28T18:46:54.631812Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>req_guid</th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15983255</td>\n",
       "      <td>Handler</td>\n",
       "      <td>[handler, ability, to, lift, 75, lbs, ability,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14382131</td>\n",
       "      <td>Receiver day shift</td>\n",
       "      <td>[receiver, day, shift, contract, role, may, be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15255153</td>\n",
       "      <td>NET Developer PCI Compliance</td>\n",
       "      <td>[net, developer, pci, compliance, our, client,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HPJP00081328</td>\n",
       "      <td>US Business Analyst 3</td>\n",
       "      <td>[us, business, analyst, 3, tec, to, genuent, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SSBK26782-1</td>\n",
       "      <td>Team Member 3 Years</td>\n",
       "      <td>[team, member, 3, years, job, description, tbd]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       req_guid                      job_title  \\\n",
       "0      15983255                        Handler   \n",
       "1      14382131            Receiver day shift    \n",
       "2      15255153   NET Developer PCI Compliance   \n",
       "3  HPJP00081328          US Business Analyst 3   \n",
       "4   SSBK26782-1           Team Member 3 Years    \n",
       "\n",
       "                                     job_description  \n",
       "0  [handler, ability, to, lift, 75, lbs, ability,...  \n",
       "1  [receiver, day, shift, contract, role, may, be...  \n",
       "2  [net, developer, pci, compliance, our, client,...  \n",
       "3  [us, business, analyst, 3, tec, to, genuent, t...  \n",
       "4    [team, member, 3, years, job, description, tbd]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define lemmatization function by using NLTK WordNetLemmatizer function\n",
    "def lemma(x):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(w.lower(), pos='v') for w in x if (w != '') and (w is not None)]\n",
    "# Apply the defined function to process job descriptions\n",
    "df['job_description'] = df.apply(lambda row: lemma(row.job_description), axis=1)\n",
    "# Show some results\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding Training with Gensim\n",
    "\n",
    "A word embedding is a form of representing words and documents using a dense vector representation. The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used. Word embeddings can be trained using the input texts. One can read more about word embeddings [here](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/) and [here](https://jalammar.github.io/illustrated-word2vec/).\n",
    "\n",
    "In this notebook, I am going to use gensim library Word2Vec functionality to generate word embedding vectors so that we can use those vectors later on to train the other models.\n",
    "\n",
    "Gensim Word2Vec will generate a vector (dimension of 300 here) for each word after training based on all job descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T19:25:57.757246Z",
     "start_time": "2020-01-28T19:25:57.671178Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare all the text input for training word2vec model\n",
    "sentences = df['job_description'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T19:46:41.905694Z",
     "start_time": "2020-01-28T19:29:32.426328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=381161, size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# Define and train a word2vec model. \n",
    "# Here I set vector dimension size to be 300, window (word distanse) to be 5 \n",
    "# and use all available CPUs for parallel processing\n",
    "model_w2v = Word2Vec(sentences, size=300, window=5, min_count=1, workers=cpu_count)\n",
    "# summarize vocabulary\n",
    "# word_vocabulary = list(model_w2v.wv.vocab)\n",
    "# print(word_vocabulary)\n",
    "# save model with binary format\n",
    "model_w2v.save('nnc_word2vec.bin')\n",
    "# load model when needed so that this word2vec model doesn't need to be re-trained\n",
    "# model_w2v = Word2Vec.load('nnc_word2vec.pkl')\n",
    "print(model_w2v)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "371.141px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
