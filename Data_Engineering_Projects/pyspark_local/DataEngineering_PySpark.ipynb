{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T18:25:29.427437Z",
     "start_time": "2021-08-25T18:25:19.454865Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.0.1\n"
     ]
    }
   ],
   "source": [
    "# This cell is not needed when this jupyter notebook is running on a Sagemaker instance\n",
    "# This is only needed when running it on local laptop\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession, HiveContext\n",
    "# Set up a spark session with leveraging all available CPUs\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master('local[*]')\\\n",
    "        .appName(\"Demo\") \\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://localhost:9083\") \\\n",
    "        .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "print(\"Spark Version: \" + spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T05:03:09.543348Z",
     "start_time": "2021-02-15T05:03:09.522214Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'local-1613358889945'),\n",
       " ('spark.driver.host', '92.242.140.21'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.port', '60997'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.name', 'Demo'),\n",
       " ('spark.driver.bindAddress', '127.0.0.1')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T18:44:19.039925Z",
     "start_time": "2021-08-25T18:44:18.516724Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, mean, udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.sql import Row, Column\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataFrame via Row and Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T03:15:32.531456Z",
     "start_time": "2021-02-15T03:15:29.044219Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| Id|Color|\n",
      "+---+-----+\n",
      "|  0|  red|\n",
      "|  1| blue|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define rows and schena\n",
    "rows = [Row(0, \"red\"), Row(1, \"blue\")]\n",
    "schema = [\"Id\", \"Color\"]\n",
    "df = spark.createDataFrame(rows, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T03:15:33.800565Z",
     "start_time": "2021-02-15T03:15:33.445647Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| Id|Color|\n",
      "+---+-----+\n",
      "|  0|  red|\n",
      "|  1| blue|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Even simpler\n",
    "rows = [(0, \"red\"), (1, \"blue\")]\n",
    "schema = \"Id INT, Color STRING\"\n",
    "df = spark.createDataFrame(rows, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T03:53:28.760365Z",
     "start_time": "2021-02-15T03:53:27.211312Z"
    }
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").saveAsTable('test_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T03:18:14.427128Z",
     "start_time": "2021-02-15T03:18:14.412622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "LogicalRDD [Id#13, Color#14], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "Id: int, Color: string\n",
      "LogicalRDD [Id#13, Color#14], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "LogicalRDD [Id#13, Color#14], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Scan ExistingRDD[Id#13,Color#14]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display Spark query logical and physical plans\n",
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HiveContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T05:09:04.694563Z",
     "start_time": "2021-02-15T05:09:04.688824Z"
    }
   },
   "outputs": [],
   "source": [
    "hive_Context = HiveContext(spark.sparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T04:39:54.077420Z",
     "start_time": "2021-02-15T04:39:54.074144Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:/Users/cheyaohu/WorkDocs/TFC/Spark/spark-warehouse/\n"
     ]
    }
   ],
   "source": [
    "print(hive_Context.getConf(\"spark.sql.warehouse.dir\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T04:38:16.823158Z",
     "start_time": "2021-02-15T04:38:16.568281Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hive_Context.sql(\"CREATE TABLE IF NOT EXISTS TestTable (key INT, value STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T04:41:28.529128Z",
     "start_time": "2021-02-15T04:41:28.473871Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "| default|  test_db|      false|\n",
      "| default|testtable|      false|\n",
      "+--------+---------+-----------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(hive_Context.sql(\"SHOW Tables\").show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Metadata in Spark Session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T03:18:19.185480Z",
     "start_time": "2021-02-15T03:18:19.049996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(spark.sql('SHOW DATABASES').show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T03:18:22.989755Z",
     "start_time": "2021-02-15T03:18:22.875065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "| default|  test_db|      false|\n",
      "+--------+---------+-----------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(spark.sql('SHOW TABLES').show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T04:02:22.690112Z",
     "start_time": "2021-02-15T04:02:22.589918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[col_name: string, data_type: string, comment: string]\n"
     ]
    }
   ],
   "source": [
    "print(spark.sql(\"DESCRIBE test_db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T04:41:40.904214Z",
     "start_time": "2021-02-15T04:41:40.626521Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='Default Hive database', locationUri='file:/Users/cheyaohu/WorkDocs/TFC/Spark/spark-warehouse')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T04:41:44.527028Z",
     "start_time": "2021-02-15T04:41:44.343774Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='test_db', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='testtable', database='default', description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+\n",
      "| Id|Color|Mark|\n",
      "+---+-----+----+\n",
      "|  1| blue| bad|\n",
      "|  0|  red|good|\n",
      "+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create two dataframes\n",
    "rows = [(0, \"red\"), (1, \"blue\")]\n",
    "schema = \"Id INT, Color STRING\"\n",
    "df = spark.createDataFrame(rows, schema)\n",
    "\n",
    "rows1 = [(0, \"good\"), (1, \"bad\")]\n",
    "schema1 = \"Id INT, Mark STRING\"\n",
    "df1 = spark.createDataFrame(rows1, schema1)\n",
    "\n",
    "tmp_df = df.join(df1, on=\"Id\", how=\"inner\")\n",
    "tmp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| Id| Color|\n",
      "+---+------+\n",
      "|  0|   red|\n",
      "|  1|  blue|\n",
      "|  3| green|\n",
      "|  4|yellow|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create two dataframes\n",
    "rows = [(0, \"red\"), (1, \"blue\")]\n",
    "schema = \"Id INT, Color STRING\"\n",
    "df = spark.createDataFrame(rows, schema)\n",
    "\n",
    "rows1 = [(3, \"green\"), (4, \"yellow\")]\n",
    "schema1 = \"Id INT, Color STRING\"\n",
    "df1 = spark.createDataFrame(rows1, schema1)\n",
    "\n",
    "tmp_df = df.union(df1)\n",
    "tmp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datetime Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T13:27:17.994546Z",
     "start_time": "2021-02-11T13:27:17.700912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|                  t|\n",
      "+-------------------+\n",
      "|1997-02-28 10:30:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T13:27:34.637266Z",
     "start_time": "2021-02-11T13:27:34.621228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- t: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T13:28:16.881367Z",
     "start_time": "2021-02-11T13:28:16.828411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.select(to_timestamp(df.t, 'yyyy-MM-dd HH:mm:ss').alias('dt'))\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T13:28:34.080044Z",
     "start_time": "2021-02-11T13:28:33.852827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|                 dt|\n",
      "+-------------------+\n",
      "|1997-02-28 10:30:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T13:30:59.389373Z",
     "start_time": "2021-02-11T13:30:59.157687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+\n",
      "|year|month|day|\n",
      "+----+-----+---+\n",
      "|1997|    2| 28|\n",
      "+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "\n",
    "df1.select(\n",
    "    year(\"dt\").alias('year'), \n",
    "    month(\"dt\").alias('month'), \n",
    "    dayofmonth(\"dt\").alias('day')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Defined Function (UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T13:44:17.480236Z",
     "start_time": "2021-02-11T13:44:17.472146Z"
    }
   },
   "outputs": [],
   "source": [
    "def date_convert(date_col, date_format='yyyy-MM-dd HH:mm:ss'):\n",
    "    date_list = date_col.split(\"-\")\n",
    "    year = date_list[0]\n",
    "    month = date_list[1]\n",
    "    day = date_list[2].split(\" \")[0]\n",
    "    return f'{month}/{day}/{year}'\n",
    "date_convert_udf = udf(lambda x: date_convert(x) if x is not None else None, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T13:47:27.328329Z",
     "start_time": "2021-02-11T13:47:27.213175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|               date|\n",
      "+-------------------+\n",
      "|1997-02-28 10:30:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['date'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T13:47:30.678943Z",
     "start_time": "2021-02-11T13:47:30.549258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|date-formatted|\n",
      "+--------------+\n",
      "|    02/28/1997|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(date_convert_udf(col('date')).alias('date-formatted') ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T13:47:33.682070Z",
     "start_time": "2021-02-11T13:47:33.606130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|               date|\n",
      "+-------------------+\n",
      "|1997-02-28 10:30:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T13:47:39.512402Z",
     "start_time": "2021-02-11T13:47:39.391397Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+\n",
      "|               date|date-formatted|\n",
      "+-------------------+--------------+\n",
      "|1997-02-28 10:30:00|    02/28/1997|\n",
      "+-------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('date-formatted', date_convert_udf(col('date')))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rename Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "|Id |Color_Name|\n",
      "+---+----------+\n",
      "|0  |red       |\n",
      "|1  |blue      |\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create two dataframes\n",
    "rows = [(0, \"red\"), (1, \"blue\")]\n",
    "schema = \"Id INT, Color STRING\"\n",
    "df = spark.createDataFrame(rows, schema)\n",
    "df = df.withColumnRenamed(\"Color\", \"Color_Name\")\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T20:02:13.260384Z",
     "start_time": "2021-01-22T20:02:13.168270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|Color|\n",
      "+---+-----+\n",
      "|  0|  Red|\n",
      "|  1| Blue|\n",
      "|  2|Green|\n",
      "|  3| null|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a data frame\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"Red\"),\n",
    "    (1, \"Blue\"),\n",
    "    (2, \"Green\"),\n",
    "    (3, None)\n",
    "], [\"id\", \"Color\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop rows with NULL values in ANY columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T20:02:17.443962Z",
     "start_time": "2021-01-22T20:02:17.352795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|id |Color|\n",
      "+---+-----+\n",
      "|0  |Red  |\n",
      "|1  |Blue |\n",
      "|2  |Green|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(\"any\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop rows with NULL values in ALL columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T20:03:25.502304Z",
     "start_time": "2021-01-22T20:03:25.370245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|id |Color|\n",
      "+---+-----+\n",
      "|0  |Red  |\n",
      "|1  |Blue |\n",
      "|2  |Green|\n",
      "|3  |null |\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(\"all\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop rows with NULL values by using ```dropna()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T20:02:40.686821Z",
     "start_time": "2021-01-22T20:02:40.577076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|id |Color|\n",
      "+---+-----+\n",
      "|0  |Red  |\n",
      "|1  |Blue |\n",
      "|2  |Green|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropna().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill the NULL Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T20:19:36.929411Z",
     "start_time": "2021-01-22T20:19:36.794162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|Color|\n",
      "+---+-----+\n",
      "|  0|  Red|\n",
      "|  1| Blue|\n",
      "|  2|Green|\n",
      "|  3| null|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a data frame\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"Red\"),\n",
    "    (1, \"Blue\"),\n",
    "    (2, \"Green\"),\n",
    "    (3, None)\n",
    "], [\"id\", \"Color\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the null values with \"NA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T20:20:11.298038Z",
     "start_time": "2021-01-22T20:20:11.186735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|Color|\n",
      "+---+-----+\n",
      "|  0|  Red|\n",
      "|  1| Blue|\n",
      "|  2|Green|\n",
      "|  3|   NA|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill('NA').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the null values with mean or average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T20:21:56.170271Z",
     "start_time": "2021-01-22T20:21:56.051737Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|  id| Color|\n",
      "+----+------+\n",
      "|   0|   Red|\n",
      "|   1|  Blue|\n",
      "|   2| Green|\n",
      "|null|Yellow|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (0, \"Red\"),\n",
    "    (1, \"Blue\"),\n",
    "    (2, \"Green\"),\n",
    "    (None, \"Yellow\")\n",
    "], [\"id\", \"Color\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T20:25:12.218741Z",
     "start_time": "2021-01-22T20:25:11.921282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id| Color|\n",
      "+---+------+\n",
      "|  0|   Red|\n",
      "|  1|  Blue|\n",
      "|  2| Green|\n",
      "|  1|Yellow|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mean_val=df.select(mean(df.id)).collect()\n",
    "df.na.fill(mean_val[0][0],subset=['id']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T15:16:55.746030Z",
     "start_time": "2021-01-22T15:16:55.560590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|Color|\n",
      "+---+-----+\n",
      "|  0|  Red|\n",
      "|  1| Blue|\n",
      "|  2|Green|\n",
      "|  3|White|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a data frame\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"Red\"),\n",
    "    (1, \"Blue\"),\n",
    "    (2, \"Green\"),\n",
    "    (3, \"White\")\n",
    "], [\"id\", \"Color\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T15:16:58.971521Z",
     "start_time": "2021-01-22T15:16:57.761693Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+-------------------+\n",
      "|id |Color|Color_Array|Color_OneHotEncoded|\n",
      "+---+-----+-----------+-------------------+\n",
      "|0  |Red  |[Red]      |(4,[3],[1.0])      |\n",
      "|1  |Blue |[Blue]     |(4,[1],[1.0])      |\n",
      "|2  |Green|[Green]    |(4,[0],[1.0])      |\n",
      "|3  |White|[White]    |(4,[2],[1.0])      |\n",
      "+---+-----+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding with Pyspark CountVectorizer\n",
    "\n",
    "df = df.withColumn(\"Color_Array\", split(col(\"Color\"),\" \"))\n",
    "colorVectorizer = CountVectorizer(inputCol=\"Color_Array\", outputCol=\"Color_OneHotEncoded\", vocabSize=4, minDF=1.0)\n",
    "colorVectorizer_model = colorVectorizer.fit(df)\n",
    "df_ohe = colorVectorizer_model.transform(df)\n",
    "df_ohe.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T15:21:16.546946Z",
     "start_time": "2021-01-22T15:21:16.454905Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the one-hot encoded column into numpy array\n",
    "\n",
    "x_3d = np.array(df_ohe.select('Color_OneHotEncoded').collect())\n",
    "X = x_3d.reshape(4, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Index Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T15:26:54.079182Z",
     "start_time": "2021-01-22T15:26:53.917001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|Color|\n",
      "+---+-----+\n",
      "|  0|  Red|\n",
      "|  1| Blue|\n",
      "|  2|Green|\n",
      "|  3|White|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a data frame\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"Red\"),\n",
    "    (1, \"Blue\"),\n",
    "    (2, \"Green\"),\n",
    "    (3, \"White\")\n",
    "], [\"id\", \"Color\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T19:00:14.541421Z",
     "start_time": "2021-01-22T19:00:13.579358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+\n",
      "|id |Color|Color_Index|\n",
      "+---+-----+-----------+\n",
      "|0  |Red  |2.0        |\n",
      "|1  |Blue |0.0        |\n",
      "|2  |Green|1.0        |\n",
      "|3  |White|3.0        |\n",
      "+---+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert a column into numerical categories (indexing)\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"Color\", outputCol=\"Color_Index\")\n",
    "labelIndexer_model = labelIndexer.fit(df)\n",
    "df_lie = labelIndexer_model.transform(df)\n",
    "df_lie.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T19:14:57.757856Z",
     "start_time": "2021-01-22T19:14:57.552149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+\n",
      "| id|Color|Color_Array|\n",
      "+---+-----+-----------+\n",
      "|  0|  Red|      [Red]|\n",
      "|  1| Blue|     [Blue]|\n",
      "|  2|Green|    [Green]|\n",
      "|  3|White|    [White]|\n",
      "+---+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a data frame\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"Red\"),\n",
    "    (1, \"Blue\"),\n",
    "    (2, \"Green\"),\n",
    "    (3, \"White\")\n",
    "], [\"id\", \"Color\"])\n",
    "df = df.withColumn(\"Color_Array\", split(col(\"Color\"),\" \"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T19:22:09.022236Z",
     "start_time": "2021-01-22T19:22:08.546492Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup one-hot encoding\n",
    "\n",
    "colorVectorizer = CountVectorizer(inputCol=\"Color_Array\", outputCol=\"Color_OneHotEncoded\", vocabSize=4, minDF=1.0)\n",
    "colorVectorizer_model = colorVectorizer.fit(df)\n",
    "df_ohe = colorVectorizer_model.transform(df)\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"Color\", outputCol=\"Color_Index\")\n",
    "labelIndexer_model = labelIndexer.fit(df_ohe)\n",
    "df_lie = labelIndexer_model.transform(df_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T19:22:45.825956Z",
     "start_time": "2021-01-22T19:22:45.674746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+-------------------+-----------+-------------------+\n",
      "|id |Color|Color_Array|Color_OneHotEncoded|Color_Index|features           |\n",
      "+---+-----+-----------+-------------------+-----------+-------------------+\n",
      "|0  |Red  |[Red]      |(4,[0],[1.0])      |2.0        |(5,[0,4],[1.0,2.0])|\n",
      "|1  |Blue |[Blue]     |(4,[3],[1.0])      |0.0        |(5,[3],[1.0])      |\n",
      "|2  |Green|[Green]    |(4,[2],[1.0])      |1.0        |(5,[2,4],[1.0,1.0])|\n",
      "|3  |White|[White]    |(4,[1],[1.0])      |3.0        |(5,[1,4],[1.0,3.0])|\n",
      "+---+-----+-----------+-------------------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vecAssembler = VectorAssembler(inputCols=[\"Color_OneHotEncoded\", \"Color_Index\"], outputCol=\"features\")\n",
    "df_va = vecAssembler.transform(df_lie)\n",
    "df_va.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T22:01:23.170744Z",
     "start_time": "2021-01-22T22:01:23.047752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Factor|Color|\n",
      "+------+-----+\n",
      "|     0| 10.0|\n",
      "|     1| 11.0|\n",
      "|     2| 12.0|\n",
      "|     3| 13.0|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a data frame\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0, 10.0),\n",
    "    (1, 11.0),\n",
    "    (2, 12.0),\n",
    "    (3, 13.0)\n",
    "], [\"Factor\", \"Color\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T22:03:43.229707Z",
     "start_time": "2021-01-22T22:03:43.024673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+-----------------------------------------+\n",
      "|Factor|Color|Color_VA  |scaledColor                              |\n",
      "+------+-----+----------+-----------------------------------------+\n",
      "|0     |10.0 |[0.0,10.0]|[-1.161895003862225,-1.161895003862225]  |\n",
      "|1     |11.0 |[1.0,11.0]|[-0.3872983346207417,-0.3872983346207417]|\n",
      "|2     |12.0 |[2.0,12.0]|[0.3872983346207417,0.3872983346207417]  |\n",
      "|3     |13.0 |[3.0,13.0]|[1.161895003862225,1.161895003862225]    |\n",
      "+------+-----+----------+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "va = VectorAssembler(inputCols=[\"Factor\",\"Color\"], outputCol=\"Color_VA\")\n",
    "df_tmp = va.transform(df)\n",
    "scaler = StandardScaler(inputCol=\"Color_VA\", outputCol=\"scaledColor\", withStd=True, withMean=True)\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(df_tmp)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "scaledData = scalerModel.transform(df_tmp)\n",
    "scaledData.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T14:04:49.352509Z",
     "start_time": "2021-07-07T14:04:49.214218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataEngineering_PySpark.html\r\n",
      "DataEngineering_PySpark.ipynb\r\n",
      "LearningSpark-v2.0.pdf\r\n",
      "Spark Streaming + Kinesis Integration - Spark 3.0.1 Documentation.pdf\r\n",
      "derby.log\r\n",
      "\u001b[34mmetastore_db\u001b[m\u001b[m\r\n",
      "scala - Cannot connect to Hive metastore from Spark application - Stack Overflow.pdf\r\n",
      "\u001b[34mspark-warehouse\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T20:39:58.778241Z",
     "start_time": "2021-08-25T20:39:58.621771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|id  |usage_data                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "+----+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|9747|\"[{\"\"a\"\":\"\"WhatsApp\"\",\"\"b\"\":1770276},{\"\"a\"\":\"\"TikTok\"\",\"\"b\"\":782435},{\"\"a\"\":\"\"Huawei Home\"\",\"\"b\"\":191391},{\"\"a\"\":\"\"Chrome\"\",\"\"b\"\":86829}]\"                                                                                                                                                                                                                                                                          |\n",
      "|9748|\"[{\"\"a\"\":\"\"Clash Royale\"\",\"\"b\"\":4305710},{\"\"a\"\":\"\"Brawl Stars\"\",\"\"b\"\":732519},{\"\"a\"\":\"\"Huawei Home\"\",\"\"b\"\":166347},{\"\"a\"\":\"\"WhatsApp\"\",\"\"b\"\":128494}]\"                                                                                                                                                                                                                                                              |\n",
      "|9778|\"[{\"\"a\"\":\"\"Brawl Stars\"\",\"\"b\"\":9226985},{\"\"a\"\":\"\"Huawei Home\"\",\"\"b\"\":5683960},{\"\"a\"\":\"\"ZEPETO\"\",\"\"b\"\":1836285},{\"\"a\"\":\"\"Head Soccer\"\",\"\"b\"\":1695973},{\"\"a\"\":\"\"Subway Surf\"\",\"\"b\"\":1661483},{\"\"a\"\":\"\"Instagram\"\",\"\"b\"\":1118615},{\"\"a\"\":\"\"World Cup Run\"\",\"\"b\"\":415267},{\"\"a\"\":\"\"WhatsApp\"\",\"\"b\"\":402191},{\"\"a\"\":\"\"Homo Evolution\"\",\"\"b\"\":163055},{\"\"a\"\":\"\"Fotocamera\"\",\"\"b\"\":100395},{\"\"a\"\":\"\"TikTok\"\",\"\"b\"\":82293}]\"|\n",
      "|9750|\"[{\"\"a\"\":\"\"Head Ball 2\"\",\"\"b\"\":12536005},{\"\"a\"\":\"\"Huawei Home\"\",\"\"b\"\":2541131},{\"\"a\"\":\"\"WhatsApp\"\",\"\"b\"\":1586903},{\"\"a\"\":\"\"Temi\"\",\"\"b\"\":1081179},{\"\"a\"\":\"\"Chrome\"\",\"\"b\"\":181637}]\"                                                                                                                                                                                                                                  |\n",
      "|9753|\"[{\"\"a\"\":\"\"Brawl Stars\"\",\"\"b\"\":16665739},{\"\"a\"\":\"\"Blockman GO\"\",\"\"b\"\":10466750},{\"\"a\"\":\"\"Gacha Life\"\",\"\"b\"\":557026},{\"\"a\"\":\"\"Huawei Home\"\",\"\"b\"\":514865},{\"\"a\"\":\"\"Draw It\"\",\"\"b\"\":405699},{\"\"a\"\":\"\"Huawei Swype\"\",\"\"b\"\":89676},{\"\"a\"\":\"\"Fotocamera\"\",\"\"b\"\":68641}]\"                                                                                                                                                 |\n",
      "+----+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.options(header='True', inferSchema='True', delimiter='\\t').format(\"csv\").load(\"example.txt\").na.drop()\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T20:36:19.149074Z",
     "start_time": "2021-08-25T20:36:19.025233Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+-------+\n",
      "|id  |a           |b      |\n",
      "+----+------------+-------+\n",
      "|9747|WhatsApp    |1770276|\n",
      "|9747|TikTok      |782435 |\n",
      "|9747|Huawei Home |191391 |\n",
      "|9747|Chrome      |86829  |\n",
      "|9748|Clash Royale|4305710|\n",
      "+----+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, explode, json_tuple\n",
    "def str_to_arr(my_string):\n",
    "    my_list = my_string.replace('[', \"\").replace(']', \"\").split(\"},\", -1)\n",
    "    results = []\n",
    "    for item in my_list:\n",
    "        if item[-1] != \"}\":\n",
    "            item = item + \"}\"\n",
    "        if item[0:2] == '\"{':\n",
    "            item = item.replace('\"{', '{')\n",
    "        results.append(item.replace('\"\"', '\"'))\n",
    "    return results\n",
    "str_to_arr_udf = udf(str_to_arr, ArrayType(StringType()))\n",
    "# convert string to array\n",
    "df = df.withColumn('usage_data_arr',str_to_arr_udf(df[\"usage_data\"]))\n",
    "# split array to rows\n",
    "df = df.select(df.id,explode(df.usage_data_arr))\n",
    "# convert Json items into columns\n",
    "df.select(col(\"id\"),json_tuple(col(\"col\"),\"a\",\"b\")) \\\n",
    "    .toDF(\"id\",\"a\",\"b\") \\\n",
    "    .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
