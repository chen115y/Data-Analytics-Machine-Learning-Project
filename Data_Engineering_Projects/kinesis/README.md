1. Create a kinesis data stream: set the name to be "ml-chapter3-stream" and the number of sherd to be "1"
2. Create cloud formation stack by using [setup-data-producer.yml](./setup-data-producer.yml) and then set the stack name to be "stream-data-producer-stack" and choose the kinesis data stream just created on the step above. If the cloud formation stack was successfully created, the kinesis service should start receiving stream data.
3. Create kinesis analytics application: set the name to be "data-transformation-application" and choose runtime to be "SQL". And then connect streaming data to be the kinesis data stream just created on step 1. If everything works well, then the data schema can be seen by clicking "discovery schema".
4. Create SQL query to only select columns that needed by using the SQL query script of [create-subset-transformation-query.sql.txt](./create-subset-transformation-query.sql.txt). And then "save and run SQL".
5. When this data-transformation-application runs well and shows the query results, click "Destination" tab and then click "Connect to a destination".
6. And then choose "Kinesis Firehose delivery stream" to create a kinesis firehose application: set the delivery stream name to be "ml-chapter3-delivery-stream" and choose "Direct PUT or other sources".
7. And then, a lambda function needs to be created to have further transformation after the kinesis firehose delivery stream. Choose "General Firehose Processing" to create the lambda function. Set the function name to be "add-newline-function" and choose "Create a new role with basic Lambda permissions". And then click "Create function" to edit the function.
8. Copy and paste the content of the file [index.js](./index.js) to the lambda function code section. Set the function time out to be "1 minute".
9. Back to the kinesis firehose delivery stream and choose lambda function just created on above step. And then choose s3 bucket. And then choose s3 buffer size to be "1 MB" and buffer interval to be "60 seconds". And then choose "create new IAM role" with default setup and click finally "Create delivery stream".
10. Back to data-transformation-application to choose kinesis firehose delivery stream just created on above step and choose in-application stream name to be "DESTINATION_USER_DATA" and then click "Save and continue".
11. If everything works well, the s3 bucket should start showing the transformed json files.
